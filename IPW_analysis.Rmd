---
title: "Inverse Probability Weighted Analysis for Observational Data"
author: "Causal Inference Analysis"
date: "March 3, 2025"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: cosmo
    highlight: tango
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, 
                      fig.width = 10, fig.height = 6, dpi = 300)
```

# Introduction

This document performs causal inference analysis using inverse probability weighting (IPW) methods on an observational dataset. The analysis adjusts for confounding by weighting observations based on their propensity to receive treatment, which helps to create a pseudo-randomized scenario from observational data.

## Loading Required Libraries

```{r load-libraries}
# Load necessary libraries
library(dplyr)
library(ggplot2)
library(ipw)
library(survey)
library(cobalt)
library(tableone)
```

# Data Preparation

## Loading the Dataset

```{r load-data}
# Load the cleaned and imputed dataset
df <- read.csv("data/imputed_case_study_data.csv")

# Display the structure of the dataset
str(df)

# Show the first few rows
head(df)

# Summary statistics
summary(df[, c("age", "baseline_gfr", "bmi", "treatment", "event")])

# Check for missing values
colSums(is.na(df))
```

# Exploratory Data Analysis

Before diving into the causal analysis, let's explore the dataset to understand the distribution of key variables and their relationships with treatment assignment.

```{r eda}
# Distribution of treatment
table(df$treatment)
prop.table(table(df$treatment))

# Distribution of outcome by treatment group
table(df$treatment, df$event)
prop.table(table(df$treatment, df$event), margin = 1)

# Create balance table before any weighting
covariates <- c("age", "baseline_gfr", "bmi", "sex", "diabetes", "hypertension")
baseline_table <- CreateTableOne(vars = covariates, strata = "treatment", data = df, test = TRUE)
print(baseline_table, smd = TRUE)
```

```{r visualization}
# Visualize the distribution of key continuous variables by treatment
# Age distribution
ggplot(df, aes(x = age, fill = factor(treatment))) +
  geom_density(alpha = 0.5) +
  labs(title = "Age Distribution by Treatment Group", 
       x = "Age", y = "Density", fill = "Treatment") +
  theme_minimal()

# Baseline GFR distribution
ggplot(df, aes(x = baseline_gfr, fill = factor(treatment))) +
  geom_density(alpha = 0.5) +
  labs(title = "Baseline GFR Distribution by Treatment Group", 
       x = "Baseline GFR", y = "Density", fill = "Treatment") +
  theme_minimal()

# BMI distribution
ggplot(df, aes(x = bmi, fill = factor(treatment))) +
  geom_density(alpha = 0.5) +
  labs(title = "BMI Distribution by Treatment Group", 
       x = "BMI", y = "Density", fill = "Treatment") +
  theme_minimal()

# Categorical variables
# Sex distribution
ggplot(df, aes(x = factor(sex), fill = factor(treatment))) +
  geom_bar(position = "dodge") +
  labs(title = "Sex Distribution by Treatment Group", 
       x = "Sex (1 = Male, 0 = Female)", y = "Count", fill = "Treatment") +
  theme_minimal()

# Diabetes distribution
ggplot(df, aes(x = factor(diabetes), fill = factor(treatment))) +
  geom_bar(position = "dodge") +
  labs(title = "Diabetes Distribution by Treatment Group", 
       x = "Diabetes (1 = Yes, 0 = No)", y = "Count", fill = "Treatment") +
  theme_minimal()

# Hypertension distribution
ggplot(df, aes(x = factor(hypertension), fill = factor(treatment))) +
  geom_bar(position = "dodge") +
  labs(title = "Hypertension Distribution by Treatment Group", 
       x = "Hypertension (1 = Yes, 0 = No)", y = "Count", fill = "Treatment") +
  theme_minimal()
```

# Step 1: Estimating the Propensity Score

We'll use logistic regression to model the probability of receiving the treatment given the observed covariates.

```{r propensity-score}
# Model treatment as a function of key covariates using logistic regression
ps_model <- glm(treatment ~ age + baseline_gfr + bmi + sex + diabetes + hypertension,
                data = df, family = binomial)

# Summary of the propensity score model
summary(ps_model)

# Obtain the predicted propensity scores (P(A=1|X))
df$pscore <- predict(ps_model, type = "response")

# Visualize propensity score distribution by treatment group
ggplot(df, aes(x = pscore, fill = factor(treatment))) +
  geom_density(alpha = 0.5) +
  labs(title = "Propensity Score Distribution by Treatment Group",
       x = "Propensity Score", y = "Density", fill = "Treatment") +
  theme_minimal()

# Check for positivity (common support)
# Create bins of propensity scores
df$ps_bin <- cut(df$pscore, breaks = seq(0, 1, 0.1))
counts <- table(df$ps_bin, df$treatment)
prop_table <- prop.table(counts, margin = 1)
print(counts)
print(prop_table)

# Visualize overlap
ggplot(df, aes(x = ps_bin, fill = factor(treatment))) +
  geom_bar(position = "stack") +
  labs(title = "Treatment Assignment by Propensity Score Bins",
       x = "Propensity Score Bins", y = "Count", fill = "Treatment") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

# Step 2: Calculate Stabilized Weights

Stabilized weights help reduce the variance of the estimator compared to unstabilized weights.

```{r stabilized-weights}
# Compute the marginal probability of treatment assignment
pA1 <- mean(df$treatment == 1)
pA0 <- 1 - pA1

# Calculate stabilized weights:
# For treated: weight = P(A=1) / pscore
# For controls: weight = P(A=0) / (1 - pscore)
df <- df %>%
  mutate(stab_weight = ifelse(treatment == 1, pA1 / pscore, pA0 / (1 - pscore)))

# Examine summary of weights
summary(df$stab_weight)

# Assess the distribution of weights
ggplot(df, aes(x = stab_weight)) +
  geom_histogram(binwidth = 0.5, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Stabilized Weights", 
       x = "Stabilized Weight", y = "Frequency") +
  theme_minimal()

# Check for extreme weights
df %>%
  summarize(
    Mean_Weight = mean(stab_weight),
    SD_Weight = sd(stab_weight),
    Median_Weight = median(stab_weight),
    Min_Weight = min(stab_weight),
    Max_Weight = max(stab_weight),
    Q1 = quantile(stab_weight, 0.25),
    Q3 = quantile(stab_weight, 0.75)
  )

# Visualize weights by treatment group
ggplot(df, aes(x = stab_weight, fill = factor(treatment))) +
  geom_density(alpha = 0.5) +
  labs(title = "Distribution of Stabilized Weights by Treatment Group", 
       x = "Stabilized Weight", y = "Density", fill = "Treatment") +
  theme_minimal()
```

# Step 3: Diagnostics for Weighting

## 3.1: Assess Covariate Balance After Weighting

```{r balance-assessment}
# Create a survey design object using the stabilized weights
design <- svydesign(ids = ~1, data = df, weights = ~stab_weight)

# Compare covariate balance before weighting using tableone
covariates <- c("age", "baseline_gfr", "bmi", "sex", "diabetes", "hypertension")
table1_before <- CreateTableOne(vars = covariates, strata = "treatment", data = df, test = FALSE)
print(table1_before, smd = TRUE)

# Assess balance after weighting using cobalt
bal_tab <- bal.tab(treatment ~ age + baseline_gfr + bmi + sex + diabetes + hypertension, 
                  data = df, weights = df$stab_weight, estimand = "ATE")
print(bal_tab)

# Visualize balance with a love plot
love.plot(bal_tab, threshold = 0.1, var.order = "unadjusted", 
          abs = TRUE, line = TRUE, colors = c("red", "blue"),
          title = "Covariate Balance Before and After Weighting")

# Create weighted means for each variable
weighted_means <- svyby(~ age + baseline_gfr + bmi + sex + diabetes + hypertension, 
                       ~treatment, design, svymean)
print(weighted_means)

# Calculate standardized mean differences after weighting
bal.tab(treatment ~ age + baseline_gfr + bmi + sex + diabetes + hypertension, 
       data = df, weights = df$stab_weight, estimand = "ATE", 
       method = "weighting", disp = c("means", "sds", "sdiffs"))

# Balance plot for each covariate
bal.plot(treatment ~ age + baseline_gfr + bmi + sex + diabetes + hypertension, 
        data = df, weights = df$stab_weight, var.name = "age")
bal.plot(treatment ~ age + baseline_gfr + bmi + sex + diabetes + hypertension, 
        data = df, weights = df$stab_weight, var.name = "baseline_gfr")
bal.plot(treatment ~ age + baseline_gfr + bmi + sex + diabetes + hypertension, 
        data = df, weights = df$stab_weight, var.name = "bmi")
```

# Step 4: Fit the Weighted Outcome Model

```{r weighted-model}
# Use the survey design to fit a weighted logistic regression model for the outcome
weighted_model <- svyglm(event ~ treatment, design = design, family = binomial)
summary(weighted_model)

# Extract coefficients and calculate odds ratio
coef_summary <- summary(weighted_model)
odds_ratio <- exp(coef(weighted_model)["treatment"])
or_ci <- exp(confint(weighted_model)["treatment", ])

# Display odds ratio and confidence interval
or_result <- data.frame(
  Odds_Ratio = odds_ratio,
  Lower_CI = or_ci[1],
  Upper_CI = or_ci[2]
)
print(or_result)

# Optional: Calculate risk differences and risk ratios
# Create a function to compute predicted probabilities
get_predicted_probs <- function(model, newdata) {
  pred <- predict(model, newdata = newdata, type = "response")
  return(mean(pred))
}

# Calculate risk for treated
risk_treated <- get_predicted_probs(weighted_model, 
                                   data.frame(treatment = rep(1, nrow(df))))

# Calculate risk for untreated
risk_untreated <- get_predicted_probs(weighted_model, 
                                     data.frame(treatment = rep(0, nrow(df))))

# Calculate risk difference and risk ratio
risk_difference <- risk_treated - risk_untreated
risk_ratio <- risk_treated / risk_untreated

results <- data.frame(
  Risk_Treated = risk_treated,
  Risk_Untreated = risk_untreated,
  Risk_Difference = risk_difference,
  Risk_Ratio = risk_ratio
)
print(results)
```

# Step 5: Sensitivity Analyses

## 5.1 Truncate Extreme Weights

```{r truncated-weights}
# Truncate weights at the 1st and 99th percentiles
lower_bound=0.01
upper_bound=0.99
df <-  df %>%
  mutate(trunc_weight = pmin(pmax(stab_weight, lower_bound), upper_bound))

# Summary of truncated weights
summary(df$trunc_weight)

# Compare original and truncated weights
ggplot(df) +
  geom_point(aes(x = stab_weight, y = trunc_weight, color = factor(treatment))) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  labs(title = "Original vs. Truncated Weights",
       x = "Original Stabilized Weights", 
       y = "Truncated Weights",
       color = "Treatment") +
  theme_minimal()

# Refit the model with truncated weights
design_trunc <- svydesign(ids = ~1, data = df, weights = ~trunc_weight)
weighted_model_trunc <- svyglm(event ~ treatment, design = design_trunc, family = binomial)
summary(weighted_model_trunc)

# Compare the results of original and truncated weights models
original_coef <- coef(weighted_model)["treatment"]
original_ci <- confint(weighted_model)["treatment", ]
original_se <- summary(weighted_model)$coefficients["treatment", "Std. Error"]

trunc_coef <- coef(weighted_model_trunc)["treatment"]
trunc_ci <- confint(weighted_model_trunc)["treatment", ]
trunc_se <- summary(weighted_model_trunc)$coefficients["treatment", "Std. Error"]

comparison <- data.frame(
  Model = c("Original", "Truncated"),
  Coefficient = c(original_coef, trunc_coef),
  SE = c(original_se, trunc_se),
  Lower_CI = c(original_ci[1], trunc_ci[1]),
  Upper_CI = c(original_ci[2], trunc_ci[2]),
  OR = c(exp(original_coef), exp(trunc_coef)),
  OR_Lower_CI = c(exp(original_ci[1]), exp(trunc_ci[1])),
  OR_Upper_CI = c(exp(original_ci[2]), exp(trunc_ci[2]))
)
print(comparison)
```

## 5.2 Using Standardized Mortality Ratio (SMR) Weights

As seen in the WIHS cohort analysis, SMR weights can be used as an alternative weighting approach.

```{r smr-weights}
# Calculate SMR weights (weighting to the overall population)
# For treated: weight = 1
# For controls: weight = pscore / (1 - pscore)
df <- df %>%
  mutate(smr_weight = ifelse(treatment == 1, 1, pscore / (1 - pscore)))

# Examine summary of SMR weights
summary(df$smr_weight)

# Create a survey design object using the SMR weights
design_smr <- svydesign(ids = ~1, data = df, weights = ~smr_weight)

# Fit the weighted outcome model using SMR weights
weighted_model_smr <- svyglm(event ~ treatment, design = design_smr, family = binomial)
summary(weighted_model_smr)

# Add SMR results to the comparison table
smr_coef <- coef(weighted_model_smr)["treatment"]
smr_ci <- confint(weighted_model_smr)["treatment", ]
smr_se <- summary(weighted_model_smr)$coefficients["treatment", "Std. Error"]

comparison_extended <- rbind(comparison, 
                           data.frame(
                             Model = "SMR",
                             Coefficient = smr_coef,
                             SE = smr_se,
                             Lower_CI = smr_ci[1],
                             Upper_CI = smr_ci[2],
                             OR = exp(smr_coef),
                             OR_Lower_CI = exp(smr_ci[1]),
                             OR_Upper_CI = exp(smr_ci[2])
                           ))
print(comparison_extended)
```

## 5.3 Augmented Inverse Probability Weighting (AIPW)

Following the approach in the causalRisk examples, we can implement an AIPW estimator that combines IPW with an outcome model.

# To do: debug and run the code below!

```{r aipw, eval=FALSE}
# This is a simplified version of AIPW
# First, fit outcome models for each treatment group
outcome_model_treated <- glm(event ~ age + baseline_gfr + bmi + sex + diabetes + hypertension, 
                           data = subset(df, treatment == 1), family = binomial)
outcome_model_control <- glm(event ~ age + baseline_gfr + bmi + sex + diabetes + hypertension, 
                           data = subset(df, treatment == 0), family = binomial)

# Predict outcomes for all individuals under both treatment scenarios
df$y1_pred %>%
  mutate(
    aipw_y1 = ifelse(treatment == 1, 
                    event + (stab_weight * (event - y1_pred)), 
                    y1_pred),
    aipw_y0 = ifelse(treatment == 0, 
                    event + (stab_weight * (event - y0_pred)), 
                    y0_pred)
  )

# Calculate AIPW treatment effect
aipw_risk_treated <- mean(df$aipw_y1)
aipw_risk_untreated <- mean(df$aipw_y0)
aipw_risk_diff <- aipw_risk_treated - aipw_risk_untreated
aipw_risk_ratio <- aipw_risk_treated / aipw_risk_untreated

aipw_results <- data.frame(
  Risk_Treated = aipw_risk_treated,
  Risk_Untreated = aipw_risk_untreated,
  Risk_Difference = aipw_risk_diff,
  Risk_Ratio = aipw_risk_ratio
)
print(aipw_results)

# Compare with IPW results
comparison_methods <- data.frame(
  Method = c("IPW", "AIPW"),
  Risk_Treated = c(risk_treated, aipw_risk_treated),
  Risk_Untreated = c(risk_untreated, aipw_risk_untreated),
  Risk_Difference = c(risk_difference, aipw_risk_diff),
  Risk_Ratio = c(risk_ratio, aipw_risk_ratio)
)
print(comparison_methods)
```

# Step 6: Additional Analyses - Competing Risks

Based on the WIHS cohort analysis, we can implement competing risks analysis. In our context, we'll assume that death or loss to follow-up could be competing risks with our primary event.

# To do: debug the code below!

```{r competing-risks, eval=FALSE}
# Create an example competing risk scenario
# Let's assume we have a variable called "competing_event" that represents death or loss to follow-up
# For demonstration purposes, we'll create it
set.seed(123)
df$competing_event <- rbinom(nrow(df), 1, 0.1) # 10% chance of a competing event

# For subjects with both main event and competing event, we'll prioritize the one that came first
# We'll assume competing events happened first for a random subset
overlap_indices <- which(df$event == 1 & df$competing_event == 1)
first_competing <- sample(overlap_indices, size = floor(length(overlap_indices) / 2))
df$event[first_competing] <- 0

# Create a composite event indicator (1 for primary event, 2 for competing event, 0 for no event)
df$composite_event <- ifelse(df$event == 1, 1, ifelse(df$competing_event == 1, 2, 0))
table(df$composite_event)

# Use multinomial regression for the competing risks analysis
library(nnet)
competing_model %
  group_by(treatment) %>%
  summarize(
    Primary_Event_Rate = mean(event),
    Competing_Event_Rate = mean(competing_event),
    Pred_Primary_Rate = mean(prob_event),
    Pred_Competing_Rate = mean(prob_competing)
  )
```

# Conclusion

In this analysis, we've performed a comprehensive inverse probability weighted (IPW) analysis to estimate the causal effect of treatment on the outcome, adjusting for observed confounding. The key steps included:

1. Estimating propensity scores
2. Calculating stabilized weights
3. Assessing covariate balance before and after weighting
4. Fitting weighted outcome models
5. Conducting sensitivity analyses with truncated weights and SMR weights
6. Advanced analyses including AIPW and competing risks (conceptual examples)

The results demonstrate the importance of proper adjustment for confounding in observational studies and the utility of IPW methods in causal inference.
