---
title: "Step 4 – Sensitivity Analyses"
author: "Andrew Mertens"
date: "2025-05-01"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---


4.1 Unmeasured confounding (E-value, bias factors)
4.2 Informative censoring (IPC weighting vs g-formula)
4.3 Alternative AKI definitions (ICD algorithm variants)
4.4 Varying overlap weights / trimming



# 4 Sensitivity Analyses

In Steps 1–3, we defined the causal estimand, established
identifiability assumptions, and outlined our primary estimation
strategy using TMLE and Super Learner. In this step, we assess the
robustness of our causal estimates to violations of these critical
assumptions through structured sensitivity analyses.

## 4.1 Importance of Sensitivity Analysis

Sensitivity analyses systematically evaluate how robust our results are
to violations of key assumptions:

-    **Positivity (overlap)**: All individuals have some chance of
    receiving either treatment.

-   **Conditional exchangeability**: No unmeasured confounding.

-   **Non-informative censoring**: Follow-up loss or treatment switching
    not related to underlying risk. We explicitly examine each
    assumption and propose sensitivity analyses to evaluate potential
    bias if these assumptions are violated.

## 4.2 Assessing Positivity Violations 

**What is positivity?** Positivity requires that every subgroup
    defined by measured covariates has a realistic chance of receiving
    either treatment. 
    
**How can positivity violations affect results?**
Lack of overlap leads to unstable estimates, large standard errors, or bias. \### Diagnostic Checks for Positivity Use propensity scores to identify regions with poor overlap:

    \$\$${r positivity-check, eval=FALSE} # Examine propensity score distributions hist(tmle_fit$likelihood$factor_list$A$eval_nodes$g1,
    breaks = 50, main = "Propensity Score Distribution (Probability of
    SOF)", xlab = "Propensity Score")
    
Sensitivity Analysis for Positivity Truncation: Limit analyses to regions of sufficient overlap (e.g., propensity scores between 0.05 and 0.95).

Alternative matching or stratification: Compare TMLE estimates within
well-overlapped subgroups.

4.3 Evaluating Unmeasured Confounding What is unmeasured confounding?
Unobserved variables related to both treatment and outcome that bias the
causal estimate.

E-value Calculation E-values quantify how strong an unmeasured
confounder must be to explain away observed effects.

Calculate E-values for estimated Risk Ratios (RR):

library(EValue) \# Example calculation, replace with your actual
estimate/confidence interval RR_estimate \<- 1.5 RR_ci_lower \<- 1.2
RR_ci_upper \<- 1.9 evalue_result \<- evalue(RR_estimate, lo =
RR_ci_lower, hi = RR_ci_upper) print(evalue_result) Interpretation: An
E-value of 2 means an unmeasured confounder would need to double the
likelihood of treatment and outcome to fully explain away our observed
association.

Negative Control Outcomes Using outcomes unaffected by treatment but
sharing confounding structure can detect residual confounding:

For example, "Urinary tract infection (UTI)" is unlikely caused by SOF
but shares similar healthcare-seeking confounders.

A significant treatment-UTI association would indicate unmeasured
confounding.

# Repeat analysis using UTI as outcome \# Compare to primary AKI results

4.4 Assessing Informative Censoring What is informative censoring?
Censoring due to treatment switching, loss to follow-up, or death
correlated with the risk of the outcome.

If censoring is informative, it biases estimates from as-treated
analyses.

Inverse Probability of Censoring Weighting (IPCW) Re-run TMLE with
censoring weights (IPCW) to correct potential bias:

# Specify TMLE with IPCW weights for censoring events tmle_fit_ipcw \<- tmle3( tmle_spec, data = dataset_long_format, node_list = node_list, learner_list = list( Y = sl_lib, A = sl_lib, C = sl_lib \# censoring model ), ipcw = TRUE ) summary(tmle_fit_ipcw)

Compare IPCW and Primary Results If results differ substantially,
informative censoring may be important, requiring deeper investigation.

4.5 Alternative Outcome Definitions (Measurement Error) Given the
potential measurement error in claims-based AKI definitions, we conduct
sensitivity analyses using alternative outcome definitions:

More stringent AKI definition: AKI requiring hospitalization or dialysis
initiation.

Less stringent definition: Any creatinine elevation from electronic
medical records (if available).

Evaluate whether estimates change substantially under different outcome
definitions.

4.6 Sensitivity to Model Specification Evaluate robustness of estimates
to alternative Super Learner specifications. Example:

# Super Learner with simpler learners simpler_sl_lib \<- list( Lrnr_glm_fast$new(), Lrnr_mean$new() ) tmle_fit_simple \<- tmle3( tmle_spec, data = dataset_long_format, node_list = node_list, learner_list = list(Y = simpler_sl_lib, A = simpler_sl_lib, C = simpler_sl_lib) ) summary(tmle_fit_simple)

If results remain consistent, findings are robust to model choices.


G-value Reporting: Introduces the concept of a "G-value," the minimal causal gap on the risk-difference scale needed to move the observed confidence interval to the null, providing stakeholders a clear quantitative metric of robustness.

Multiverse Analysis: Recommends explicitly performing sensitivity analyses varying key analytic choices (propensity-score specifications, weight truncation rules, outcome definitions), helping clarify robustness of causal estimates to analytic decisions.


4.7 Summary of Sensitivity Analyses Sensitivity analyses provide
assurance of robustness or help uncover weaknesses requiring attention.
A structured approach addressing potential violations of positivity,
unmeasured confounding, censoring assumptions, measurement error, and
model specification strengthens credibility of the causal findings.

Sensitivity Analyses Purpose Method(s) Positivity Ensure valid
comparisons across groups Propensity score overlap diagnostics,
truncation Unmeasured Confounding Assess robustness to unknown
confounders E-values, negative controls Informative Censoring Evaluate
censoring bias IPCW, alternative censoring definitions Measurement Error
Robustness of outcome measurement Alternative outcome definitions Model
Specification Robustness of statistical modeling choices Alternative
Super Learner libraries

Next Step: Step 5 – Interpretation of Findings and Reporting. We'll
integrate sensitivity results with primary estimates, discussing
practical implications and conclusions clearly for stakeholders.
