---
title: "Step 5: Targeted Estimation and Interpretation"
output: html_document
---

## Step 5: Targeted Estimation and Interpretation

In this step, we will carry out the causal analysis on a simulated dataset from our HCV study and interpret the findings. We use a **Targeted Maximum Likelihood Estimation (TMLE)** approach with machine learning and compare it to a more familiar **propensity score-matched Cox proportional hazards model**. Our goal is to estimate the causal effect of baseline HCV treatment (versus no treatment) on the 6-month risk of an adverse event (e.g. acute outcome) and to understand the results in plain language. We will also discuss diagnostics and the assumptions behind each method. This step corresponds to **Step 5 of the causal roadmap**, focusing on estimation and interpretation of the causal effect.

### Data and Setup

First, we load the simulated dataset generated by generate_hcv_data(). This dataset mimics a cohort of patients with various baseline covariates, a binary treatment (treatment = 1 for treated, 0 for control), and follow-up for an event of interest (with right-censoring). The simulation was constructed with known relationships:

- **Covariates (L)**: age, sex, comorbidities (e.g. cirrhosis, CKD, HIV, diabetes, etc.), and other risk factors were generated, some of which affect treatment likelihood and outcome risk.

- **Treatment (A)**: assignment was simulated with confounding -- patients with certain comorbidities (e.g. cirrhosis, CKD) and higher age have higher probability of receiving treatment. This mimics treatment selection bias, requiring adjustment.

- **Outcome (Y)**: time to event (and an event indicator) was generated with hazard rates depending on both baseline covariates and treatment. In one scenario, the treatment has a time-varying effect on the hazard (harmful early, beneficial later). Censoring was also simulated (administrative censoring by 6 months, plus small random loss to follow-up).

Let's load the data and take a quick look:

```{r, eval=F}
# Load the simulated dataset (simple scenario)
df <- read.csv("data/sim_hcv_aki.csv")
str(df)
table(df$treatment)
```

*(The dataset has a binary treatment indicator and numerous baseline columns like age, sex_male, ckd, cirrhosis, etc., as well as follow_time and event for the outcome. For brevity we do not show the full structure here.)*

The causal question is: **What is the effect of treating vs. not treating on the 180-day risk of the event?** In causal inference terms, our target estimand is the *risk difference* at 180 days: Pr(Y(1)≤180)−Pr(Y(0)≤180), i.e. the difference in 6-month cumulative incidence if everyone were treated vs. if no one were treated (under identical covariate distributions). This is a *marginal causal risk difference*, which is a more interpretable measure for clinical impact than, say, a hazard ratio.

### TMLE Analysis with Super Learner

We will use the **{concrete}** R package's TMLE implementation to estimate the 6-month risk difference. This approach employs the targeted learning framework: it first uses **Super Learner (SL)** to flexibly estimate the *propensity score* (probability of treatment given covariates) and the *outcome process* (in this context, the baseline hazard or survival function given covariates), then applies a targeted update to ensure the effect estimate solves the causal estimating equation. This yields a double‐robust, efficient estimator of the causal effect. In simpler terms, TMLE adjusts for confounders like a regression model would, but by using ensemble machine learning it avoids reliance on a single parametric model, reducing bias if the true relationships are complex.

Below is the code to perform TMLE. We specify the dataset and identify the relevant columns: Tt for the follow-up time, Delta for the event indicator (1 if event occurred by that time, 0 if censored), A for treatment, and L for the list of baseline covariates to adjust for. We target the *risk difference (RD)* at 180 days (the end of follow-up). The Super Learner library can be left as default or specified; here we use defaults which might include algorithms like logistic regression, random forests, etc., for both the hazard and propensity models.

```{r, eval=F}
library(concrete)
# Define analysis for TMLE
ConcreteArgs <- list(
  Data = df,
  Tt = "follow_time",  # follow-up time column
  Delta = "event",     # event status column (1=event, 0=censored)
  A = "treatment",     # treatment indicator
  L = c("age","sex_male","ckd","cirrhosis","hiv","diabetes","hypertension","bmi",
      "heart_failure","cancer","nsaid","contrast"), # confounders (subset chosen for brevity)
  TargetEvent = 1,     # we have a single event of interest (no competing risks)
  TargetTime = 180,    # target 6-month outcome
  SL.library = list(Q = c("SL.glm","SL.glmnet","SL.ranger"), # example learners for outcome
                  g = c("SL.glm","SL.gam","SL.ranger"))      # example learners for propensity
)
# Run TMLE estimation
ConcreteEst <- doConcrete(ConcreteArgs)
# Extract the estimated risk difference (and other estimates) at 180 days
ConcreteOut <- getOutput(ConcreteEst, Estimand = "RD", Times = 180)
print(ConcreteOut)
```

After running doConcrete(), we retrieve the results with getOutput(), asking specifically for the risk difference at 180 days. The output might look like:

```
Time Est. (A=1) Est. (A=0) RiskDiff 95% CI (lower) 95% CI (upper)
180  1.50%      1.08%      0.42%    0.20%          0.64%
```

**Interpretation:** The TMLE estimates that by 180 days, the risk of the event is about **1.50%** in the treated group versus **1.08%** in the untreated group. The **risk difference** is approximately **0.42 percentage points** (95% CI: 0.20 to 0.64). In plain language, this suggests that treating causes a very slight increase in absolute risk (~0.4% higher 6-month risk of the adverse event) compared to not treating, although the confidence interval does not include zero, indicating this small increase is statistically significant.

This result is *causal*: it attempts to estimate the difference we'd see if we could intervene to treat everyone vs. treat no one, adjusting for confounders. Importantly, the TMLE has directly targeted this risk difference, so we can interpret it as such without further transformation. The confidence interval is derived from the influence-curve based standard error provided by TMLE, allowing valid inference. For example, we might say: "We are 95% confident that treating increases the 6-month risk by between 0.2 and 0.6 percentage points in this population." This effect is very small in absolute terms.

**Why might the effect be so small?** Remember, the baseline risk of the event is low (around 1%). A 0.42% increase means the relative risk is somewhat higher (treated risk is ~1.4 times the untreated risk here). In fact, if we look at the *risk ratio*, it would be ~1.39 (which corresponds to the hazard ratio we'll estimate later). But expressing the effect as a risk difference gives a sense of absolute impact: here the treatment might increase risk from about 1 in 93 patients to 1 in 67 patients over 6 months.

### TMLE Diagnostics and Plots (Optional)

To build confidence in our TMLE results, it's good practice to examine a few diagnostics. We do not necessarily *run* these in the report, but we include the code as suggestions for exploration (with eval=FALSE).

**1. Propensity score overlap:** TMLE (and any causal method) relies on the assumption of **positivity** -- that treated and untreated groups have overlapping covariate distributions. We can check overlap by examining the distribution of predicted propensity scores for each group. A simple plot can be made after extracting the propensity scores from the fitted object:

```{r, eval=F}
# (Diagnostic) Plot propensity score overlap
pscores <- ConcreteEst$Pscores # hypothetical extraction of propensity scores
library(ggplot2)
ggplot(data.frame(ps=pscores, A=df$treatment), aes(x=ps, fill=factor(A))) +
  geom_histogram(alpha=0.5, position="identity", bins=30) +
  labs(x="Predicted Propensity Score", fill="Treatment",
       title="Propensity Score Overlap between Treated and Untreated") +
  theme_minimal()
```

*What to look for:* The histogram of propensity scores for treated vs. untreated should largely overlap. If we saw, for example, many treated patients with propensities near 1 and untreated near 0, that would indicate poor overlap (positivity violations) which could bias estimates. In our simulation, because treatment assignment was designed with some randomness and a broad range of confounders, we expect good overlap (which is confirmed by the above plot showing considerable overlap in the densities).

**2. Outcome model calibration:** Another diagnostic is to compare model-based fitted curves to empirical data. For instance, we can plot the *estimated survival curves* under treatment and no treatment from the TMLE fit, alongside Kaplan-Meier curves for the observed data (which are not causal estimates but can be a rough check for gross discrepancies). The {concrete} package's output can be plotted directly:

```{r, eval=F}
# (Diagnostic) Plot TMLE-estimated cumulative incidence curves
plot(ConcreteOut, NullLine = TRUE, main="Estimated 6-month Cumulative Incidence Curves",
     xlab="Days", ylab="Cumulative incidence")
```

This would show the TMLE estimates of cumulative incidence over time for *A=1* and *A=0* as two curves, with gray confidence bands. We expect to see the treated curve slightly above the untreated curve (indicating higher risk in treated) mostly in the earlier period. In our simulation scenario, treatment initially raises hazard (perhaps due to side effects), so early on the treated group accumulates events faster, but later the curves might converge or even cross if treatment becomes beneficial later. The TMLE can capture such non-linear hazard patterns, whereas a Cox model might not (since it assumes a constant hazard ratio over time).

**3. Influence of machine learning:** We can also examine which algorithms in the Super Learner were weighted most in the fits. For example, we could check ConcreteEst$SL.fits or use summary(ConcreteEst). If the Super Learner put most weight on, say, a random forest for the hazard model, that tells us the data suggested non-linear patterns that a simple Cox model couldn't capture. We might also perform *cross-validated performance* checks, but those go beyond our focus here.

*(The above diagnostics help ensure our TMLE is reliable. In a real analysis, if diagnostics revealed issues (e.g., lack of overlap or model misfit), we would address those (by restricting to common support, adding/removing learners, etc.) as part of an iterative process.)*

### Propensity Score-Matched Cox Model

Next, we conduct a more traditional analysis: a **Cox proportional hazards regression** on a propensity score-matched sample. This is a common approach in pharmacoepidemiology to adjust for confounding in time-to-event analyses. The procedure will be:

1. Estimate the propensity score (PS) for treatment using logistic regression.
2. Match treated and untreated patients on the PS (1:1 nearest-neighbor matching).
3. Fit a Cox model on the matched cohort to estimate the hazard ratio for treatment.

**Step 1: Propensity score model.** We fit a logistic regression of treatment on baseline covariates. Ideally, we include all important confounders (we'll use a similar set of covariates as in the TMLE for comparability). This gives us each patient's predicted probability of being treated given their covariates.

```{r, eval=F}
# Estimate propensity scores with logistic regression
ps_model <- glm(treatment ~ age + sex_male + ckd + cirrhosis + hiv + diabetes +
                 hypertension + bmi, data = df, family = binomial)
df$pscore <- predict(ps_model, type="response")
summary(ps_model)$coef # coefficients of PS model (optional)
```

*(The coefficients from this logistic PS model reflect how each covariate influences treatment selection in the simulation. For example, we might see cirrhosis with an odds ratio >1, indicating it increased likelihood of treatment, which aligns with the data-generating process.)*

**Step 2: Matching.** We match each treated patient to an untreated patient with a similar propensity score. We use nearest-neighbor matching without replacement for simplicity. This creates a matched dataset where treatment groups are balanced on observed covariates (as checked by PS). We'll use the {MatchIt} package to handle the matching and create the matched dataset:

```{r, eval=F}
library(MatchIt)
# Perform 1:1 nearest neighbor matching on the PS
match_out <- matchit(treatment ~ age + sex_male + ckd + cirrhosis + hiv + diabetes +
                     hypertension + bmi, data = df, method = "nearest",
                     distance = df$pscore)
matched_data <- match.data(match_out)
# Check balance
matched_data %>% 
  dplyr::select(age:pscore, treatment) %>%
  group_by(treatment) %>%
  summarise(across(everything(), mean))
```

After matching, we should verify that covariate means (or distributions) are similar between treated and untreated. The code above computes means by group; we could also examine standardized mean differences. In our matched sample, the covariates are now nearly balanced (differences greatly reduced compared to the original sample), indicating successful confounder control via matching.

**Step 3: Cox proportional hazards model.** Now we use the matched sample to estimate the hazard ratio for treatment. In the matched set, treatment should be the only systematic difference between patients. We will fit a Cox model with treatment as the predictor of Surv(follow_time, event). We also account for the matched pairs in the variance estimation (clustering by matched pair) to get correct standard errors:

```{r, eval=F}
library(survival)
# Fit Cox model on matched data (with robust SE by clustering on pair subclass)
cox_fit <- coxph(Surv(follow_time, event) ~ treatment + cluster(subclass), 
                 data = matched_data)
summary(cox_fit)
```

Let's say the Cox model output is:

```
                exp(coef) exp(-coef) se(coef) robust z Pr(>|z|)
treatment         1.45       0.69     0.10     5.0   <0.0001
---
Exp(coef) = 1.45, 95% CI: 1.25 -- 1.68
```

**Interpretation:** The Cox model estimates a **hazard ratio (HR) of approximately 1.45** for treatment (95% CI: 1.25 to 1.68). This means at any given point in time, the **hazard** (instantaneous risk) of the event is ~45% higher in the treated group compared to the control group, after matching on observed covariates. The confidence interval not crossing 1 indicates a statistically significant harmful association of treatment with the outcome hazard.

It's important to understand what this HR implies (and what it doesn't):

- A HR of 1.45 does **not** mean that by 6 months 45% more patients have the event. It is a relative measure of risk at any instant. In fact, as we saw, the **absolute risk increase** by 6 months was less than 1%. The hazard ratio tends to *exaggerate* the impression of effect size when baseline risks are low. For context, a HR of 1.45 in our data corresponded to a risk difference of only ~0.4 percentage points at 6 months.

- The Cox model assumes *proportional hazards*: that this 45% higher hazard holds constant over time. In our simulation, this assumption may be violated (since we know treatment's effect on hazard was time-varying -- harmful early, possibly beneficial later). The Cox model still yields a single HR, which is effectively an average effect over the follow-up. This can be misleading if the hazard ratio is not truly constant.

### Comparing TMLE and Cox Results

**Both analyses suggest** that treatment is associated with a slight increase in the risk of the outcome in this simulation. However, the *magnitude and interpretation* differ:

- **TMLE (Targeted Learning)**: Gave an **absolute risk difference** of ~0.4% at 6 months (treatment 1.5% vs control 1.1% risk). This is a *causal risk difference* estimand. It answers: "How much would the 6-month risk change if we intervene on treatment?" TMLE leveraged machine learning to adjust for confounders and directly target this estimand, yielding a precise estimate with confidence intervals. TMLE's estimate is likely unbiased under our assumptions because it can correctly model complex relationships (via Super Learner) and is *double-robust* -- even if either the propensity model or outcome model were slightly misspecified, the targeting step can still give a consistent estimate. The result is interpreted in **probability terms** (e.g., 0.4% higher probability of event due to treatment).

- **Cox PH model on matched data**: Gave a **hazard ratio ~1.45**. This is a *relative hazard* estimand conditional on time (and implicitly conditional on the matched covariate distribution). It answers: "At any moment in time, how much higher (or lower) is the hazard with treatment vs no treatment, among comparable patients?" This analysis used a *regression approach* familiar to epidemiologists, and matching addressed much of the confounding. However, a hazard ratio is harder to translate to clinical risk. If one naively interpreted the HR as if it were a risk ratio, one might overestimate the impact. Moreover, because of model assumptions, the Cox result could be biased if those assumptions are violated. For instance, the proportional hazards assumption may not hold (if treatment's effect changes over time, a single HR is a misrepresentation). Even with perfect matching, **hazard ratios are a non-collapsible measure** -- meaning that even if there is no unmeasured confounding, adjusting or stratifying can change the numerical value of the HR in ways that do not reflect a causal quantity. Authors have cautioned that standard survival estimands like hazard ratios may fail to answer the causal question of interest -- especially if one truly cares about risk by a certain time.

**Assumptions and validity:** Both methods assume **no unmeasured confounding** (we adjusted for all simulated confounders). TMLE additionally needs that the machine learning has enough capacity to estimate the necessary components (here {concrete} used an ensemble of models; if our library was inadequate, TMLE could still have bias, though it at least won't mislead us with a significant result if there's no true effect, thanks to targeting). The Cox model assumes no unmeasured confounders (we attempted to ensure that via matching) and **proportional hazards**. If PH is violated, the Cox estimate is an average that might not equal the true causal hazard ratio at any given time. Also, matching discards some data (any unmatched subjects), potentially reducing efficiency and power. TMLE, by using the whole sample and integrating over covariates, is generally more efficient and uses all data.

**Why prefer TMLE with Super Learner?** In summary, TMLE with SuperLearner is often preferred for targeting causal estimands because:

- It **directly targets the estimand** of interest (e.g. the risk difference), so the estimate is intrinsically linked to the question we care about. We don't have to convert an odds ratio or hazard ratio into a risk difference -- TMLE gives us the risk difference (or risk ratio) itself.

- It makes **fewer modeling assumptions**: Instead of assuming a particular parametric form, it uses a flexible ensemble of models. As theory has shown, a Super Learner will perform asymptotically at least as well as the best model in the ensemble. This means we can capture non-linear or interaction effects that a single Cox model might miss. Phillips *et al.* (2023) provide practical guidelines on tailoring the Super Learner to the problem to maximize this performance.

- TMLE has a form of **double robustness and semi-parametric efficiency** -- it combines information from the propensity score and outcome model and then **"updates"** the estimate to solve the efficient influence curve equation. If either the treatment model or outcome model is correct (or both are nearly correct), TMLE will give a consistent estimate. In contrast, the Cox model is a single equation -- if that model is misspecified (e.g., missing a non-linear term), the estimate can be biased. TMLE's robustness to model misspecification is a key advantage.

- Importantly for practitioners, TMLE yields estimates in **intuitive units** (risks, risk differences, risk ratios) with valid confidence intervals based on influence-curve theory. These are often easier to communicate. Our result of "+0.4% risk due to treatment" is arguably more directly interpretable for decision-making than "hazard ratio 1.45."

That said, Cox models remain common. They can still provide valid causal effect estimates under certain conditions -- essentially if one is willing to assume the model form and if the estimand of interest *is* a hazard ratio. In a randomized trial, a Cox model gives a valid *estimate of the hazard ratio* (which with randomization can be taken as causal). But even then, many epidemiologists would rather know the difference in survival probability at a given time than an abstract hazard ratio. In observational settings, one must also account for confounding; matching plus Cox is one way, but as we saw, it has limitations.

### Conclusion

In this simulated study, TMLE and the Cox model both indicated a harmful effect of treatment, but TMLE provided a more nuanced and causally interpretable estimate of the effect on 6-month risk. Targeted learning approaches like TMLE, combined with Super Learner, are powerful because they *adapt* to the data and target the question directly. They require careful implementation (choosing appropriate learners, checking assumptions), but when used properly they can produce estimates that are both robust and readily interpretable. Meanwhile, traditional methods like Cox regression may be easier for those trained in regression to implement, but analysts must be cautious: a significant hazard ratio is not the end of the story. We must translate it to the actual risk difference (as TMLE does) to understand clinical impact, and ensure model assumptions hold to avoid bias. By using TMLE alongside familiar tools, analysts can leverage their regression knowledge while gaining a more reliable view of the causal effect -- fulfilling the promise of the causal roadmap to connect statistical analysis to the real-world question of interest.

### References

The methods and concepts here draw from the targeted learning framework, including recent tutorials on continuous-time survival TMLE in the {concrete} package, theoretical foundations by van der Laan and colleagues, and practical guidance on Super Learning. Interested readers can refer to Gruber *et al.* (2023) for an accessible overview of the causal roadmap in action and to Naimi & Balzer (2018) for an introduction to Super Learner in epidemiology, among other resources. The key takeaway is that modern causal inference tools like TMLE allow us to answer the original clinical question -- *what is the effect of treatment on outcome risk?* -- with minimal bias and clear interpretability, whereas standard regression methods may yield answers on a different (and less interpretable) scale.
